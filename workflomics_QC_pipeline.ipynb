{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super organised modular notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install the packages needed using the following command `pip install -r requirements.txt`  where the file contains the following:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "aiohttp==3.9.5\\\n",
    "asyncio==3.4.3\\\n",
    "igraph\\\n",
    "requests\\\n",
    "py4cytoscape==1.9.0\\\n",
    "pandas\\\n",
    "tqdm==4.66.2\\\n",
    "numpy\n",
    "</div>\n",
    "\n",
    "Please note that Cytoscape has to be installed and open on your machine for visualisation to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd         \n",
    "import os\n",
    "from tqdm import tqdm       \n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# define format, and set parameters for time, TODO: rm?  \n",
    "date_format = \"%Y_%m_%d_%H_%M\" \n",
    "current_date = datetime.now().strftime(date_format)\n",
    "last_week_date = (datetime.now() - timedelta(days=7)).strftime(date_format)\n",
    "\n",
    "\n",
    "\n",
    "# Project specific packages\n",
    "import aiohttp              # Used for aggregating requests into single session\n",
    "import asyncio              # -\"-\n",
    "import nest_asyncio         # For jupyter asyncio compatibility \n",
    "nest_asyncio.apply()        # Automatically takes into account how jupyter handles running event loops\n",
    "\n",
    "#import jsonpath_ng.ext      # More efficient json processing TODO: look into if actually computationally more efficient \n",
    "import igraph               # Used to create te citationa graph \n",
    "import requests             # For single API requests \n",
    "\n",
    "# Visualisation \n",
    "import py4cytoscape as p4c  # Cytoscape for visualisation of the citaton graph, \n",
    "                            # note that Cytoscape has to be installed and open on your machine for visualisation to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the packages are installed and functional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "igraph version: 0.11.4\n",
      "You are connected to Cytoscape!\n",
      "Cytoscape version: 3.10.2\n"
     ]
    }
   ],
   "source": [
    "print(\"igraph version:\", igraph.__version__)\n",
    "try: \n",
    "    dir(p4c)\n",
    "    p4c.cytoscape_ping() \n",
    "    print(\"Cytoscape version:\",p4c.cytoscape_version_info()['cytoscapeVersion'])    \n",
    "except:\n",
    "    print (\"Make sure to have Cytoscape installed and open (or don't, if you don't care about the visualisation)!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    Function description \\n    \\n    Parameters\\n    ----------\\n    name : type\\n        description\\n'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format: \n",
    "\"\"\"\n",
    "\n",
    "    Function description \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    name : type\n",
    "        description\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_biotools_page(session, url):\n",
    "    \"\"\" \n",
    "    Sync the bio.tools (page) requests so they are all made in a single session \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    session : aiohttp.client.ClientSession object\n",
    "        session object for package aiohttp\n",
    "    url : str\n",
    "        url for request\n",
    "    \"\"\"\n",
    "    \n",
    "    async with session.get(url) as response:\n",
    "        return await response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_biotools_metadata(topicID=\"topic_0121\"):  # TODO: I removed format. Check if there is any reason to have it \n",
    "                                                        # TODO: should add parameter for optional forced retrieval - even if csv file, still recreate it \n",
    "                                                        # TODO: Currently no timing - add tracker\n",
    "    \"\"\"\n",
    "    Fetches metadata about tools from bio.tools, belonging to a given topicID and returns as a dataframe.\n",
    "    If a CSV file already exists load the dataframe from it. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topicID : str TODO: make this a int instead? why am I writing topic? \n",
    "        The ID to which the tools belongs to, ex. \"Proteomics\" or \"DNA\" as defined by \n",
    "        EDAM ontology (visualisation: https://edamontology.github.io/edam-browser/#topic_0003)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    date_format = \"%Y%m%d\"\n",
    "\n",
    "    # Define the CSV filename\n",
    "    csv_filename = f'biotools_metadata_{topicID}_{datetime.now().strftime(date_format)}.csv' \n",
    "\n",
    "    # Check if the file exists and if it's older than a week\n",
    "    if os.path.isfile(csv_filename):\n",
    "        file_date = datetime.strptime(csv_filename.split('_')[-1].split('.')[0], date_format)\n",
    "        if file_date < datetime.now() - timedelta(days=7):\n",
    "            print(\"Old datafile. Updating...\")\n",
    "        else:\n",
    "            print(\"Bio.tools data loaded from existing CSV file.\")\n",
    "            df = pd.read_csv(csv_filename)\n",
    "            return df\n",
    "    else:\n",
    "        print(\"No existing bio.tools CSV file. Downloading data.\") \n",
    "    \n",
    "    # TODO: should filepath/name be allowed to be configurable?\n",
    "    # then the following could be a separate function called by this one, or is this very inefficient?\n",
    "    # TODO: should place files created in a folder named for each run\n",
    "\n",
    "    all_tool_data = [] # TODO: predefine the length, means one more request \n",
    "\n",
    "    # start at page 1 \n",
    "    page = 1 \n",
    "\n",
    "    # requests are made during single session\n",
    "    async with aiohttp.ClientSession() as session: \n",
    "        while page:\n",
    "\n",
    "            # send request for tools on the page, await further requests \n",
    "            biotools_url = f'https://bio.tools/api/t?topicID=%22{topicID}%22&format=json&page={page}'\n",
    "            biotool_data = await fetch_biotools_page(session, biotools_url)\n",
    "            \n",
    "\n",
    "            # TODO: Do I need to check? what happens if no response for page == 1? Maybe try/except instead\n",
    "            # Checking if there are any tools, if \n",
    "            if 'list' in biotool_data: \n",
    "                biotools_lst = biotool_data['list']\n",
    "\n",
    "\n",
    "                for tool in biotools_lst:\n",
    "                    name = tool.get('name') \n",
    "                    publication = tool.get('publication')\n",
    "                    topic = tool.get('topic')\n",
    "                    # TODO: decide whether we even want this information since it would only be used for visualisation and possibly teh multitopic graph \n",
    "\n",
    "                    # TODO: if no pmid, use doi converter \n",
    "                    # TODO: Download primary (using jasonpatg-ng), \n",
    "                    if name and publication and publication[0].get('pmid') and topic and topic[0].get('term'): \n",
    "                        all_tool_data.append({ #predefine, since max length == nr_tools, then need to define this earlier\n",
    "                            'name': name,\n",
    "                            'pmid': str(publication[0]['pmid']), # making sure they are all strings\n",
    "                            'topic': topic[0]['term']\n",
    "                        })\n",
    "\n",
    "                page = biotool_data.get('next')\n",
    "                if page: # else page will be None and loop will stop \n",
    "                    page = page.split('=')[-1] # only want the page number \n",
    "            else: \n",
    "                print(f'Error while fetching tool names from page {page}')\n",
    "                break\n",
    "\n",
    "    # Convert list of dictionaries to dataframe\n",
    "    df = pd.DataFrame(all_tool_data)\n",
    "    # Save dataframe to file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # If there were any pages, check how many tools were retrieved and how many tools had pmids\n",
    "    if biotool_data: \n",
    "        nr_tools = int(biotool_data['count']) \n",
    "        print(f'Found {len(all_tool_data)} out of a total of {nr_tools} tools with PMIDS.')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def europepmc(article_id, format='JSON', source='MED', page=1, page_size=1000):   # TODO: replace own wrapper with recommendation? https://github.com/ML4LitS/CAPITAL/tree/main\n",
    "                                                                                # TODO: call output=\"idlist\" immidiately? then we have no metadata but we dont use that anyways!\n",
    "    \"\"\" \n",
    "    Downloads pmids for the articles citing the given article_id, returns list of citation pmids (PubMed IDs)\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    article_id : str # TODO: int? \n",
    "        pmid, PubMed ID, for a given article.\n",
    "    source: str\n",
    "        source ID as given by the EuropePMC API documentation: https://europepmc.org/Help#contentsources \n",
    "\n",
    "    page, int, default == 1\n",
    "        determines where to start looking TODO: remove this, why would you not start at 1? \n",
    "\n",
    "    pagesize, int, default 1000 max 1000\n",
    "        determines number of results per page\n",
    "    \n",
    "    \"\"\" \n",
    "\n",
    "    # create a url with the given requirements according to the EuropePMC API synthax and query the API\n",
    "    base_url = f'https://www.ebi.ac.uk/europepmc/webservices/rest/{source}/{article_id}/citations?page={page}&pageSize={page_size}&format={format}'\n",
    "    result = requests.get(base_url)\n",
    "\n",
    "    # Return all citations, given the query was accepted\n",
    "    # TODO: jsonpath-ng\n",
    "    if result.ok:\n",
    "        return result.json()['citationList']['citation']\n",
    "    else:\n",
    "        print('Something went wrong') # TODO: better error message. Try/except? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function reate_citation_network can be called to create or load the cocitation network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: all of the descriptions - Where do I put default value? \n",
    "\n",
    "def create_citation_network(topicID=\"topic_0121\", testSize=None, randomSeed=42, loadData=True, filePath='', saveFiles=True): # TODO: I just threw  code into this function- improve\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a citation network given a topic and returns a graph and the tools included in the graph\n",
    "\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    topicID : str, default \"topic_0121\" (proteomics) TODO: int? \n",
    "        The ID to which the tools belongs to, ex. \"Proteomics\" or \"DNA\" as defined by \n",
    "        EDAM ontology (visualisation: https://edamontology.github.io/edam-browser/#topic_0003)\n",
    "\n",
    "    testSize : int or None, default None\n",
    "        Determines the number of tools included in the citation graph.\n",
    "\n",
    "    randomSeed : int, default 42\n",
    "        Specifies what seed is used to randomly pick tools in a test run. \n",
    "    \n",
    "    loadData : Boolean, default True\n",
    "        Determines if already generated graph is loaded or if it is recreated.\n",
    "    \n",
    "    filePath : str  TODO: add filepath \n",
    "        Path to already generated graph\n",
    "\n",
    "    saveFiles : Boolean, default True\n",
    "        Determines if newly generated graph is saved. \n",
    "\n",
    "    \"\"\"\n",
    "    # Retrieve the data \n",
    "    # run the asynchronous function for single session requests \n",
    "    result = asyncio.run(get_biotools_metadata(topicID=topicID)) \n",
    "    pmids = result['pmid'].tolist() # should I use numpy for all my lists? \n",
    "\n",
    "    # Randomly picks out a subset of the pmids\n",
    "    if testSize:   \n",
    "        print(f\"Creating test-cocitation network of size {testSize}. Random seed is {randomSeed}.\")\n",
    "        np.random.seed(randomSeed)\n",
    "        pmids = np.random.choice(pmids,testSize)\n",
    "\n",
    "    \n",
    "    # Edge creation \n",
    "    # Load previously created data or recreate it\n",
    "    if loadData: # TODO: pickle mayebe is not the way to go in future? \n",
    "        \n",
    "        if os.path.isfile('edges{testSize}.pkl') and os.path.isfile('graph{testSize}.pkl') and os.path.isfile('included_tools{testSize}.pkl'): # should give option to specify these names\n",
    "            print(\"Loading data\")\n",
    "            with open(f'edges{testSize}.pkl', 'rb') as f:\n",
    "                unq_edges = pickle.load(f) # should be unique ones right \n",
    "            with open(f'graph{testSize}.pkl', 'rb') as f:\n",
    "                G = pickle.load(f) \n",
    "            with open(f'included_tools{testSize}.pkl', 'rb') as f:\n",
    "                included_tools = pickle.load(f) \n",
    "        else:\n",
    "            print(f\"Files not found. Please check that 'edges{testSize}.pkl', 'graph{testSize}.pkl' and 'included_tools{testSize}.pkl' are in your current directory and run again. Or set loadData = False, to create the files. \")\n",
    "            return \n",
    "   \n",
    "    else:\n",
    "        # edge creation using europepmc\n",
    "        print(\"Downloading citation data from Europepmc.\")\n",
    "        \n",
    "        # this is to create a list of the tools that actually had citations, otherwise they are not included in the graph. \n",
    "        included_tools = []  # TODO: is there a smarter way of generating the included_tools list? \n",
    "        edges = []\n",
    "\n",
    "        # Get citations for each tool, and generate edges between them. \n",
    "        for pmid in tqdm(pmids, desc=\"Processing PMIDs\"): \n",
    "            pmid = str(pmid) # EuropePMC requires str            \n",
    "    \n",
    "            citations = europepmc(pmid, page_size=1000)\n",
    "            for citation in citations:\n",
    "                edges.append((pmid, str(citation['id']))) # TODO: this is the wring way around? shoudl be citation to pmid, no? \n",
    "                if pmid not in included_tools:\n",
    "                    included_tools.append(pmid) \n",
    "        \n",
    "        print(\"Creating citation graph using igraph.\")\n",
    "        \n",
    "        # Finding unique edges by converting list to a set (because tuples are hashable) and back to list.\n",
    "        # TODO: maybe not super efficient? \n",
    "        unq_edges =  list(set(edges)) \n",
    "        print(f\"{len(unq_edges)} unique out of {len(edges)} edges total!\")\n",
    "\n",
    "        # Creating a directed graph with unique edges\n",
    "        G = igraph.Graph.TupleList(unq_edges, directed=True)\n",
    "\n",
    "        # TODO: its starting to get messy man\n",
    "        # Removing disconnected vertices (that are not tools) that do not have information value for the (current) metric\n",
    "        print(\"Removing citations with degree less or equal to 1 (Non co-citations).\")\n",
    "        vertices_to_remove = [v.index for v in G.vs if v.degree() <= 1 and v['name'] not in included_tools] \n",
    "        G.delete_vertices(vertices_to_remove)\n",
    "        vertices_to_remove = [v.index for v in G.vs if v.degree() == 0 ] # second run to remove the copletely detatched ones after first run sicne they wont give info anyways. \n",
    "        G.delete_vertices(vertices_to_remove) # This will remove isolated tools as well \n",
    "\n",
    "        # Updating included_tools to only contain lists that are in the graph  \n",
    "        included_tools = [tool for tool in included_tools if tool in G.vs['name']] # TODO: since I am doing this anyways I could do it with pmids directly and not generate the included_tools list earier?\n",
    "\n",
    "\n",
    "        # Saving edges, graph and tools included in the graph \n",
    "        if saveFiles:\n",
    "            print(f\"Saving data to 'edges{testSize}.pkl', 'graph{testSize}.pkl' and 'included_tools{testSize}.pkl'.\") # sould make these filenames dynamic\n",
    "            # and save them \n",
    "            #Do this nicer later? \n",
    "            with open(f'edges{testSize}.pkl', 'wb') as f:\n",
    "                pickle.dump(unq_edges, f)\n",
    "\n",
    "            with open(f'graph{testSize}.pkl', 'wb') as f:\n",
    "                pickle.dump(G, f)\n",
    "\n",
    "            with open(f'included_tools{testSize}.pkl', 'wb') as f:\n",
    "                pickle.dump(included_tools, f)    \n",
    "\n",
    "    # returns a graph and the pmids of the tools included in the graph (tools connected by cocitations)\n",
    "    return G, included_tools \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the cocitation network specifying if you want to load existing data, run on a smaller test set etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bio.tools data loaded from existing CSV file.\n",
      "Creating test-cocitation network of size 100. Random seed is 42.\n",
      "Downloading citation data from Europepmc.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PMIDs: 100%|██████████| 100/100 [00:30<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating citation graph using igraph.\n",
      "5651 unique out of 6703 edges total!\n",
      "Removing citations with degree less or equal to 1 (Non co-citations).\n",
      "Saving data to 'edges100.pkl', 'graph100.pkl' and 'included_tools100.pkl'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "G, included_tools = create_citation_network(testSize=100, loadData=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download workflow data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>OBS:</b> This is not yet implemented. Currently drawing random tools to simulate workflows\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools in pseudo WF: ['12403597' '23051804' '26335203' '26510693' '34395100' '36414245']\n",
      "Generated workflow pairs (WF edges):\n",
      "('26510693', '12403597')\n",
      "('36414245', '23051804')\n",
      "('34395100', '26335203')\n"
     ]
    }
   ],
   "source": [
    "# TODO: download workflows\n",
    "\n",
    "# TODO: improve randomisation to have sequential networks\n",
    "\n",
    "# number of edges in the workflow\n",
    "num_pairs = 3\n",
    "\n",
    "workflow_pairs = []\n",
    "while len(workflow_pairs) < num_pairs:\n",
    "    article1 = np.random.choice(included_tools)\n",
    "    article2 = np.random.choice(included_tools)\n",
    "    if article1 != article2:  # Ensure article1 and article2 are different\n",
    "        workflow_pairs.append((article1, article2))\n",
    "\n",
    "workflow_tools = np.unique([element for tuple in workflow_pairs for element in tuple])\n",
    "print( \"Tools in pseudo WF:\", workflow_tools)\n",
    "# Print the generated pairs\n",
    "print(\"Generated workflow pairs (WF edges):\")\n",
    "for pair in workflow_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>OBS:</b> This is a simple placeholder implementation of a metric\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comet(graph, workflows): # cocitation metric \n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the cocitation  metric for a given workflow and a given cocitation graph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : igraph.Graph\n",
    "        Graph generated by igraph\n",
    "    workflow, list of tuples with pairs of strings TODO: how do I write this? does it matter I wont have this format later anyways? \n",
    "        List of tuples of strings corresponding to the edges in the workflow. \n",
    "\n",
    "    \"\"\"\n",
    "    # List to  collect pairwise scores\n",
    "    score_list = [] # TODO: can predefine the list length, does not matter this is temporary?\n",
    "\n",
    "    for pair in workflows:\n",
    "        cocite_score = 0\n",
    "        neighbors_of_first = set(graph.neighbors(pair[0]))\n",
    "        neighbors_of_second = set(graph.neighbors(pair[1]))\n",
    "\n",
    "        # Count number of common neighbours\n",
    "        common_neighbors = neighbors_of_first.intersection(neighbors_of_second)\n",
    "        cocite_score = len(common_neighbors)\n",
    "        score_list.append(cocite_score)\n",
    "\n",
    "    # Then sum the scores or perform any other desired calculation\n",
    "    # now normalising by WF length\n",
    "    # maybe call this one \"support\", since that is basically what we have. \n",
    "\n",
    "    return sum(score_list)/len(score_list), score_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "Total cocitation score for workflow [('26510693', '12403597'), ('36414245', '23051804'), ('34395100', '26335203')] is 0.3333333333333333.\n"
     ]
    }
   ],
   "source": [
    "metric_score, raw_results = comet(G, workflow_pairs)\n",
    "\n",
    "print(raw_results)\n",
    "\n",
    "print(f\"Total cocitation score for workflow {workflow_pairs} is {metric_score}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cytoscape to visualise the network (Alma add colouring of the workflow too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying default style...\n",
      "Applying preferred layout\n",
      "Styling graph\n",
      "style_name not specified, so updating \"default\" style.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "style_name not specified, so updating \"default\" style.\n",
      "style_name not specified, so updating \"default\" style.\n",
      "style_name not specified, so updating \"default\" style.\n",
      "style_name not specified, so updating \"default\" style.\n",
      "style_name not specified, so updating \"default\" style.\n",
      "style_name not specified, so updating \"default\" style.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import time # TODO: i switched to datetime, fix this later \n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%Y_%m_%d_%H_%M\", t)\n",
    "\n",
    "\n",
    "p4c.create_network_from_igraph(G, f\"Cocitations_Size{len(included_tools)}_{current_time}\")\n",
    "\n",
    "\n",
    "print(\"Styling graph\")\n",
    "p4c.set_node_shape_default(\"ELLIPSE\")\n",
    "p4c.set_node_width_default(30)\n",
    "p4c.set_node_height_default(30)\n",
    "p4c.set_node_border_color_default(\"#000000\")  # Black color in hexadecimal\n",
    "p4c.set_node_border_width_default(1)\n",
    "p4c.set_node_color_bypass(included_tools, \"#FF0000\")  # Red color in hexadecimal\n",
    "p4c.set_node_size_bypass(included_tools, 100)  # remember it cannot be a np.array, just a list argh \n",
    "\n",
    "# colour the tools in the workflow \n",
    "p4c.set_node_color_bypass(list(workflow_tools), \"#2F739A\")  # they should be gradually coloured later maybe, sequentially\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workflomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
